{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ishandutta/audio-ml-tutorial-part-1-time-domain-features-w-b?scriptVersionId=121597097\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<h1><center>Audio ML Tutorial Part-1: Time Domain Features</center></h1>\n                                                      \n<center><img src = \"https://i.natgeofe.com/n/a189dd67-bc78-4716-aa29-cdbaceb5e4d0/photo-ark-parrots-endangered-bird-world-intelligence-3_3x2.jpg\" width = \"750\" height = \"500\"/></center>                                                                          ","metadata":{}},{"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.</center></h3>","metadata":{}},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents</center></h2>","metadata":{}},{"cell_type":"markdown","source":"<ul style=\"list-style-type:square\">\n    <li><a href=\"#1\">Preliminaries</a></li>\n    <li><a href=\"#2\">Global Config</a></li>\n    <li><a href=\"#3\">Load Datasets</a></li>\n    <li><a href=\"#4\">Weights and Biases</a></li>\n    <li><a href=\"#5\">Basic Analysis</a></li>\n    <li><a href=\"#6\">Time Domain Features</a></li>\n</ul>\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Preliminaries</center></h2>","metadata":{}},{"cell_type":"code","source":"#### Basics\n\nimport os\nimport sklearn\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\n\n#### PyTorch\nimport torch\nimport torchaudio\n\n#### Data Visualization\nimport seaborn as sns\nimport plotly.express as px \nimport IPython.display as ipd \nimport matplotlib.pyplot as plt\n%matplotlib inline \n\n#### Librosa\nimport librosa \nimport librosa.display \n\n#### Aesthetics\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n#### Logging\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2023-03-07T21:46:49.932413Z","iopub.execute_input":"2023-03-07T21:46:49.933706Z","iopub.status.idle":"2023-03-07T21:46:49.949571Z","shell.execute_reply.started":"2023-03-07T21:46:49.933614Z","shell.execute_reply":"2023-03-07T21:46:49.948109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Global Config</center></h2>","metadata":{}},{"cell_type":"code","source":"config = {\"competition\": \"BirdCLEF2023\",\n          \"_wandb_kernel\": \"ishandutta\",\n          \"sample_rate\": 32000\n          }","metadata":{"execution":{"iopub.status.busy":"2023-03-07T21:44:41.320729Z","iopub.execute_input":"2023-03-07T21:44:41.321152Z","iopub.status.idle":"2023-03-07T21:44:41.32955Z","shell.execute_reply.started":"2023-03-07T21:44:41.321117Z","shell.execute_reply":"2023-03-07T21:44:41.328036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets</center></h2>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/birdclef-2023/train_metadata.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:31:26.181314Z","iopub.execute_input":"2023-03-07T20:31:26.182119Z","iopub.status.idle":"2023-03-07T20:31:26.287766Z","shell.execute_reply.started":"2023-03-07T20:31:26.182064Z","shell.execute_reply":"2023-03-07T20:31:26.286431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases</center></h2>","metadata":{}},{"cell_type":"markdown","source":"<center><img src = \"https://i.imgur.com/1sm6x8P.png\" width = \"750\" height = \"500\"/></center>        ","metadata":{}},{"cell_type":"markdown","source":"**Weights & Biases** is the machine learning platform for developers to build better models faster.\n\nYou can use W&B's lightweight, interoperable tools to\n\n- quickly track experiments,\n- version and iterate on datasets,\n- evaluate model performance,\n- reproduce models,\n- visualize results and spot regressions,\n- and share findings with colleagues.\n  \nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations and logging seamlessly.","metadata":{}},{"cell_type":"code","source":"# Initialise the Run\n\nrun = wandb.init(project=config['competition'], job_type='Visualization', name='BirdCLEF 2023 Audio')","metadata":{"execution":{"iopub.status.busy":"2023-03-07T21:42:49.838228Z","iopub.execute_input":"2023-03-07T21:42:49.839481Z","iopub.status.idle":"2023-03-07T21:43:33.580385Z","shell.execute_reply.started":"2023-03-07T21:42:49.839434Z","shell.execute_reply":"2023-03-07T21:43:33.576203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here is a minimal example on how you can log the audio to your wandb dashboard \n\n# Initialise a table with the column names\ntable = wandb.Table(columns=['Audio Sample', 'Primary Label'])\n\n# For simplicity I have selected 100 rows only, you can have the entire dataframe\nminimal_df = df.sample(100).reset_index(drop=True)\n\n# Log the data to table\nfor i in tqdm(range(len(minimal_df))):\n    row = minimal_df.loc[i]\n    audio = wandb.Audio(row.full_path, sample_rate=config['sample_rate'])\n    table.add_data(audio, row.primary_label)\n\nwandb.log({'BirdCLEF 2023 Audio': table})\n\n# Finish the run\nrun.finish()","metadata":{"execution":{"iopub.status.busy":"2023-03-07T21:47:14.385751Z","iopub.execute_input":"2023-03-07T21:47:14.386164Z","iopub.status.idle":"2023-03-07T21:47:27.924746Z","shell.execute_reply.started":"2023-03-07T21:47:14.386129Z","shell.execute_reply":"2023-03-07T21:47:27.923451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [Interactive W&B Table for Audio $\\rightarrow$](https://wandb.ai/ishandutta/BirdCLEF2023/runs/4rvhdvlv?workspace=user-ishandutta)\n\n<center><img src = \"https://i.ibb.co/PzCngK6/Screenshot-2.png\" width = \"1000\" height = \"750\"/></center>       ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Basic Analysis</center></h2>","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Get Audio Samples</span>** ","metadata":{}},{"cell_type":"code","source":"df.primary_label.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:31:26.292267Z","iopub.execute_input":"2023-03-07T20:31:26.29314Z","iopub.status.idle":"2023-03-07T20:31:26.304059Z","shell.execute_reply.started":"2023-03-07T20:31:26.293084Z","shell.execute_reply":"2023-03-07T20:31:26.302866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.primary_label.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_audio_sample(df: pd.DataFrame, bird_label: str):\n    \"\"\"\n    Function to get the audio sample for a bird using it's label\n    \n    Args:\n        df (pandas.DataFrame): DataFrame with the metadata\n        birl_label (str): Label of the bird for which audio sample is required\n    \"\"\"\n    AUDIO_DIR_PATH = \"/kaggle/input/birdclef-2023/train_audio\"\n\n    df['full_path'] = AUDIO_DIR_PATH  + '/' + df['filename']\n    \n    return df[df['primary_label'] == bird_label].sample(1, random_state = 42)['full_path'].values[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:31:26.328894Z","iopub.execute_input":"2023-03-07T20:31:26.330042Z","iopub.status.idle":"2023-03-07T20:31:26.33791Z","shell.execute_reply.started":"2023-03-07T20:31:26.329995Z","shell.execute_reply":"2023-03-07T20:31:26.336702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abethr1_file = get_audio_sample(df, 'abethr1')\nblcapa2_file = get_audio_sample(df, 'blcapa2')\nchibat1_file = get_audio_sample(df, 'chibat1')\ndotbar1_file = get_audio_sample(df, 'dotbar1')","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:31:26.339407Z","iopub.execute_input":"2023-03-07T20:31:26.339974Z","iopub.status.idle":"2023-03-07T20:31:26.383728Z","shell.execute_reply.started":"2023-03-07T20:31:26.339926Z","shell.execute_reply":"2023-03-07T20:31:26.382545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ipd.Audio(abethr1_file)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:31:26.387406Z","iopub.execute_input":"2023-03-07T20:31:26.388018Z","iopub.status.idle":"2023-03-07T20:31:26.400583Z","shell.execute_reply.started":"2023-03-07T20:31:26.387966Z","shell.execute_reply":"2023-03-07T20:31:26.399444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ipd.Audio(blcapa2_file)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:31:26.402157Z","iopub.execute_input":"2023-03-07T20:31:26.402662Z","iopub.status.idle":"2023-03-07T20:31:26.416606Z","shell.execute_reply.started":"2023-03-07T20:31:26.402613Z","shell.execute_reply":"2023-03-07T20:31:26.415235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ipd.Audio(chibat1_file)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:31:26.418024Z","iopub.execute_input":"2023-03-07T20:31:26.41894Z","iopub.status.idle":"2023-03-07T20:31:26.439047Z","shell.execute_reply.started":"2023-03-07T20:31:26.418902Z","shell.execute_reply":"2023-03-07T20:31:26.437934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ipd.Audio(dotbar1_file)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:31:26.440589Z","iopub.execute_input":"2023-03-07T20:31:26.440923Z","iopub.status.idle":"2023-03-07T20:31:26.451248Z","shell.execute_reply.started":"2023-03-07T20:31:26.440892Z","shell.execute_reply":"2023-03-07T20:31:26.450141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"> ### **<span style=\"color:orange;\">This notebook is inspired by the [Audio Signal Processing For Machine Learning Series](https://www.youtube.com/playlist?list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0) by Valerio Velardo.  </span>** ","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Load Audio Signal</span>** ","metadata":{}},{"cell_type":"code","source":"# Using librosa we will load these audio files\n# These files are downsampled at 32Khz, which we pass\n# to the sr argument of librosa.load or we can pass None\n\nabethr1, sr = librosa.load(abethr1_file, sr=None)\nblcapa2, sr = librosa.load(blcapa2_file, sr=None)\nchibat1, sr = librosa.load(chibat1_file, sr=None)\ndotbar1, sr = librosa.load(dotbar1_file, sr=None)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:33:33.586646Z","iopub.execute_input":"2023-03-07T20:33:33.587083Z","iopub.status.idle":"2023-03-07T20:33:33.720003Z","shell.execute_reply.started":"2023-03-07T20:33:33.587048Z","shell.execute_reply":"2023-03-07T20:33:33.718825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abethr1_file","metadata":{"execution":{"iopub.status.busy":"2023-03-07T21:08:18.032192Z","iopub.execute_input":"2023-03-07T21:08:18.033134Z","iopub.status.idle":"2023-03-07T21:08:18.040489Z","shell.execute_reply.started":"2023-03-07T21:08:18.033086Z","shell.execute_reply":"2023-03-07T21:08:18.03921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abethr1.size","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:39:32.968591Z","iopub.execute_input":"2023-03-07T20:39:32.969015Z","iopub.status.idle":"2023-03-07T20:39:32.976147Z","shell.execute_reply.started":"2023-03-07T20:39:32.968977Z","shell.execute_reply":"2023-03-07T20:39:32.974982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tip: If you want to load 5 seconds of a file \n# which starts from 10 seconds use offset and duration parameters\n\nabethr1_5, sr = librosa.load(abethr1_file, sr=None, offset=15.0, duration=5.0)\nabethr1_5.size","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:39:28.764454Z","iopub.execute_input":"2023-03-07T20:39:28.765469Z","iopub.status.idle":"2023-03-07T20:39:28.797027Z","shell.execute_reply.started":"2023-03-07T20:39:28.765425Z","shell.execute_reply":"2023-03-07T20:39:28.795861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For now let us focus on abethr1\nprint(abethr1)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:40:17.893159Z","iopub.execute_input":"2023-03-07T20:40:17.893588Z","iopub.status.idle":"2023-03-07T20:40:17.900678Z","shell.execute_reply.started":"2023-03-07T20:40:17.893547Z","shell.execute_reply":"2023-03-07T20:40:17.899436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Every element of this array is actually a sample of the audio\n# The value corresponding to that audio sample is the amplitude w.r.t that sample\n\nprint(f\"abethr1 has a total of {len(abethr1)} samples\")","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:45:03.558279Z","iopub.execute_input":"2023-03-07T20:45:03.558729Z","iopub.status.idle":"2023-03-07T20:45:03.569467Z","shell.execute_reply.started":"2023-03-07T20:45:03.55867Z","shell.execute_reply":"2023-03-07T20:45:03.565498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Calculate Audio Duration</span>** ","metadata":{}},{"cell_type":"code","source":"# To find the duration of 1 sample, we take inverse of sampling rate\nsample_duration = 1/sr\nprint(f\"Duration of 1 sample of audio: {sample_duration:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:45:26.56376Z","iopub.execute_input":"2023-03-07T20:45:26.56436Z","iopub.status.idle":"2023-03-07T20:45:26.573565Z","shell.execute_reply.started":"2023-03-07T20:45:26.564302Z","shell.execute_reply":"2023-03-07T20:45:26.57219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now via ipd.Audio we already saw that the duration of abethr1 is 19 secs\n# We can also calculate it as\n\nduration = sample_duration * len(abethr1)\nprint(f\"Duration of Audio Signal: {duration} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-03-07T20:46:39.927165Z","iopub.execute_input":"2023-03-07T20:46:39.928346Z","iopub.status.idle":"2023-03-07T20:46:39.935081Z","shell.execute_reply.started":"2023-03-07T20:46:39.928297Z","shell.execute_reply":"2023-03-07T20:46:39.933671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Waveforms</span>** ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(17, 15))\n\nbirds = {\n    'abethr1': abethr1, \n    'blcapa2': blcapa2, \n    'chibat1': chibat1, \n    'dotbar1': dotbar1}\n\nfor i, (bird_name, bird_arr) in enumerate(birds.items()):\n    plt.subplot(2, 2, i+1)\n    # Librosa has an inbuilt function to display the plot directly\n    librosa.display.waveshow(bird_arr)\n    plt.title(str(bird_name))\n    plt.ylim((-1,1))","metadata":{"execution":{"iopub.status.busy":"2023-03-07T21:03:23.865015Z","iopub.execute_input":"2023-03-07T21:03:23.865439Z","iopub.status.idle":"2023-03-07T21:03:26.531917Z","shell.execute_reply.started":"2023-03-07T21:03:23.865399Z","shell.execute_reply":"2023-03-07T21:03:26.530587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As an exercise, listen to the audio signals again and focus on the portions where the waveforms show spikes.  \nAre you able to listen the spikes in the audio?\n  \nAnother interesting thing to note is that the `blcapa2` audio is a lot more soothing and has observable patterns than the `dotbar1`. Can we say that the audio for `blcapa2` is a song and that for `dotbar1` is not?\n\nLet us find out!","metadata":{}},{"cell_type":"code","source":"def get_audio_type(df: pd.DataFrame, full_audio_path: str):\n    \"\"\"\n    Function to get the audio type from the audio path\n    \n    Args:\n        df (pd.DataFrame): Metadata for the audio signals\n        full_audio_path (str): File path for the audio signal\n        \n    Returns:\n        type of audio signal\n    \"\"\"\n    \n    return df[df['full_path'] == full_audio_path]['type'].values[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-07T21:15:06.110852Z","iopub.execute_input":"2023-03-07T21:15:06.111344Z","iopub.status.idle":"2023-03-07T21:15:06.117803Z","shell.execute_reply.started":"2023-03-07T21:15:06.111299Z","shell.execute_reply":"2023-03-07T21:15:06.11633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Type of Audio for blcapa2: {get_audio_type(df, blcapa2_file)}\")\nprint(f\"Type of Audio for dotbar1: {get_audio_type(df, dotbar1_file)}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-07T21:16:32.477922Z","iopub.execute_input":"2023-03-07T21:16:32.478311Z","iopub.status.idle":"2023-03-07T21:16:32.489787Z","shell.execute_reply.started":"2023-03-07T21:16:32.478278Z","shell.execute_reply":"2023-03-07T21:16:32.488149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus our thought process was correct!","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Time Domain Features</center></h2>","metadata":{}},{"cell_type":"markdown","source":"On a high level this includes three main features:\n1. Amplitude Envelope (AE)\n2. Root Mean Square Energy (RMS)\n3. Zero Crossing Rate (RMS)","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Amplitude Envelope (AE)</span>** ","metadata":{}},{"cell_type":"markdown","source":"Amplitude Envelope gives us the envelope (boundaries) of the sound. What is the boundary for a single frame? It is it's maximum amplitude.\n  \nHence, the aim of amplitude envelope is to get the maximum amplitude for each frame. But why is this helpful?\n  \nIntuitively, the amplitude of an audio signal is indicative of how loud the audio is or simply what is it's volume. \n  \nTo obtain the AE, we split the audio signal into multiple windows, each having the same size. Then for each of the windows we find the maximum amplitude amongst the constituent frames in it. \n\nAn interesting application of AE is for onset detection, or the detection of the beginning of a sound. ","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Root Mean Square Energy (RMS)</span>** ","metadata":{}},{"cell_type":"markdown","source":"One key problem with the Amplitude Envelope is that it is very sensitive to outliers. To tackle this we have another time domain feature called the Root Mean Squared Energy. It is simply the Root Mean Squared of all the samples in a frame. \n  \nThis is an indicator of loudness as well but is much less sensitive to outliers as compared to Amplitude Envelope.","metadata":{}},{"cell_type":"markdown","source":"<h1><center>More Plots coming soon!</center></h1>\n                                                      \n<center><img src = \"https://static.wixstatic.com/media/5f8fae_7581e21a24a1483085024f88b0949a9d~mv2.jpg/v1/fill/w_934,h_379,al_c,q_90/5f8fae_7581e21a24a1483085024f88b0949a9d~mv2.jpg\" width = \"750\" height = \"500\"/></center> ","metadata":{}},{"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.</center></h3>","metadata":{}}]}